{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbc9764",
   "metadata": {},
   "source": [
    "# Generation of Expert-tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATASET_DIR = Path(\"datasets\")\n",
    "ATTRIBUTES_DIR = Path(\"other_resources/attributes\")\n",
    "EXTERNAL_DIR = Path(\"other_resources/external_resource\")\n",
    "TEMP_DESCRIPTIONS_FILE = \"paddy_disease_desc.jsonl\" \n",
    "FINAL_OUTPUT_FILE = \"paddy_disease.jsonl\" \n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    path = Path(file_path)\n",
    "    \n",
    "    if not path.exists():\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Try UTF-8 first\n",
    "    try:\n",
    "        return path.read_text(encoding=\"utf-8\").strip()\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Try latin-1\n",
    "    try:\n",
    "        return path.read_text(encoding=\"latin-1\").strip()\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Fallback: decode raw bytes while ignoring errors\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return f.read().decode(errors=\"ignore\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read file: {file_path} with all methods. Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def call_ollama(model: str, prompt: str, image_path: str = None):\n",
    "    if image_path:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model],\n",
    "            input=json.dumps({\"prompt\": prompt, \"images\": [str(Path(image_path).resolve())]}).encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE\n",
    "        )\n",
    "    else:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE\n",
    "        )\n",
    "    return result.stdout.decode(\"utf-8\")\n",
    "\n",
    "# Stage 1: Generate image descriptions using LLaVA\n",
    "def generate_descriptions():\n",
    "    # print(\"desc\")\n",
    "    with open(TEMP_DESCRIPTIONS_FILE, \"w\", encoding=\"utf-8\") as desc_out:\n",
    "        for dataset_dir in DATASET_DIR.iterdir():\n",
    "            \n",
    "            if not dataset_dir.is_dir():\n",
    "                \n",
    "                continue\n",
    "            dataset_name = dataset_dir.name\n",
    "\n",
    "            for class_dir in dataset_dir.iterdir():\n",
    "                if not class_dir.is_dir():\n",
    "                    continue\n",
    "                class_label = class_dir.name\n",
    "                attr_path = ATTRIBUTES_DIR / dataset_name / f\"{class_label}.txt\"\n",
    "                attributes = read_txt_file(attr_path)\n",
    "                # for image_path in class_dir.glob(\"*.jpg\"):\n",
    "\n",
    "                for image_path in (p for p in class_dir.iterdir() if p.suffix.lower() in [\".jpg\",\".jpeg\"]):\n",
    "                    # print(\"asd\")\n",
    "                    # print(image_path)\n",
    "                    print(f\"[Stage 1] Processing image for description: {image_path}\")\n",
    "                    try:\n",
    "                        prompt = (\n",
    "                            f\"You are an agricultural assistant. Describe this image of a {class_label} from the {dataset_name} dataset. \"\n",
    "                            f\"Use the following attributes for a more detailed and contextual description:\\n{attributes}\"\n",
    "                        )\n",
    "                        description = call_ollama(\"llava:13b\", prompt, image_path=image_path)\n",
    "                        desc_out.write(json.dumps({\n",
    "                            \"image_path\": image_path.relative_to(DATASET_DIR).as_posix(),\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"class\": class_label,\n",
    "                            \"attributes\": attributes,\n",
    "                            \"description\": description.strip()\n",
    "                        }, ensure_ascii=False) + \"\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating description for {image_path}: {e}\")\n",
    "    # print(\"done\")\n",
    "# Stage 2: Generate multi-turn Q&A using Mistral\n",
    "def generate_multiturn_qa():\n",
    "    # print(\"ml\")\n",
    "\n",
    "    results = []\n",
    "    with open(TEMP_DESCRIPTIONS_FILE, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            class_label = data[\"class\"]\n",
    "            dataset_name = data[\"dataset\"]\n",
    "            ext_path = EXTERNAL_DIR / dataset_name / f\"{class_label}.txt\"\n",
    "            external_knowledge = read_txt_file(ext_path)\n",
    "\n",
    "            print(f\"[Stage 2] Generating multi-turn QA for: {data['image_path']}\")\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "You are an AI assistant specialized in agricultural topics. You are provided with the text\n",
    "description of an image of a plant, attributes of the plant (such as name, disease),\n",
    "and common information of the plant. Unfortunately, you don't have access to the actual\n",
    "image.\n",
    "\n",
    "You must generate exactly 3 to 5 pairs of question and answer (Q&A). Each question should begin with \"Q:\" and each answer with \"A:\". Do not include any narrative text outside the Q&A pairs.\n",
    "\n",
    "Instructions:\n",
    "- Focus on visual details that can be seen in the image (e.g., plant type, symptoms, disease, prevention).\n",
    "- Do not refer to the 'text', 'context', or 'caption' — behave as if you are only seeing the image.\n",
    "- Do not ask speculative or ambiguous questions.\n",
    "- Avoid referencing numbers, scientific names, or datasets.\n",
    "- Maintain consistent formatting as:\n",
    "  Q1: ...\n",
    "  A1: ...\n",
    "  Q2: ...\n",
    "  A2: ...\n",
    "  (and so on)\n",
    "\n",
    "Context:\n",
    "Image Description: {data[\"description\"]}\n",
    "Attributes: {data[\"attributes\"]}\n",
    "External Knowledge: {external_knowledge}\n",
    "\"\"\"\n",
    "                multiturn = call_ollama(\"mistral\", prompt)\n",
    "                data[\"external_knowledge\"] = external_knowledge\n",
    "                data[\"multi_turn_conversation\"] = multiturn.strip()\n",
    "                results.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating multi-turn QA for {data['image_path']}: {e}\")\n",
    "    # print(\"ml\")\n",
    "    return results\n",
    "\n",
    "# Stage 3: Generate simple Q&A using Mistral and save final output\n",
    "def generate_simple_qa_and_save(results):\n",
    "    # print(\"sa\")\n",
    "\n",
    "    with open(FINAL_OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for data in results:\n",
    "            print(f\"[Stage 3] Generating simple QA for: {data['image_path']}\")\n",
    "            try:\n",
    "                prompt = f\"\"\"\n",
    "You are an AI assistant specialized in agricultural topics. You are provided with the text\n",
    "description of an image of a plant, attributes of the plant (such as name, disease),\n",
    "and common information of the plant. Unfortunately, you don't have access to the actual\n",
    "image.\n",
    "You are a helpful tutor. Based on the image of a {data['class']} from the {data['dataset']} dataset, generate 3–5 basic question–answer pairs.\n",
    "\n",
    "Instructions:\n",
    "- Start each question with \"Q:\" and answer with \"A:\".\n",
    "- Keep answers very short — just the name or label (like \"Tomato\", \"Late blight\", etc).\n",
    "- Do not use full sentences or long explanations in answers.\n",
    "- produce to the point answer do not produce any kind of explanation\n",
    "\n",
    "Context:\n",
    "Image Description: {data['description']}\n",
    "Attributes: {data['attributes']}\n",
    "External Knowledge: {data['external_knowledge']}\n",
    "\"\"\"\n",
    "                simple_qa = call_ollama(\"mistral\", prompt)\n",
    "                data[\"simple_qa\"] = simple_qa.strip()\n",
    "\n",
    "                outfile.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating simple QA for {data['image_path']}: {e}\")\n",
    "    # print(\"sa\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Stage 1: Generating image descriptions with LLaVA:13b...\")\n",
    "    generate_descriptions()\n",
    "\n",
    "    print(\"\\nStage 2: Generating multi-turn questions with Mistral...\")\n",
    "    multiturn_results = generate_multiturn_qa()\n",
    "\n",
    "    print(\"\\nStage 3: Generating simple QA with Mistral...\")\n",
    "    generate_simple_qa_and_save(multiturn_results)\n",
    "\n",
    "    print(\"\\nDataset generation complete. Results saved to:\", FINAL_OUTPUT_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
